## TR:
# Proje 3: LLM Destekli Chatbot (MLOps ile LLM UygulamasÄ±)

Bu proje, MLOps yaÅŸam dÃ¶ngÃ¼sÃ¼nÃ¼n modern BÃ¼yÃ¼k Dil Modelleri (LLM) alanÄ±na nasÄ±l uyarlandÄ±ÄŸÄ±nÄ± gÃ¶stermektedir. Bu uygulamada, `model_train.py` gibi geleneksel bir eÄŸitim adÄ±mÄ± yoktur; bunun yerine, yerel olarak Ã§alÄ±ÅŸan Ã¶nceden eÄŸitilmiÅŸ bir LLaMA modeli (`Ollama` aracÄ±lÄ±ÄŸÄ±yla) kullanÄ±lÄ±r. `LangChain` kÃ¼tÃ¼phanesi ile hafÄ±za yÃ¶netimi saÄŸlanÄ±r, `FastAPI` ile bir sohbet API'si oluÅŸturulur ve `Streamlit` ile kullanÄ±cÄ± arayÃ¼zÃ¼ sunulur.

---

## Projenin AmacÄ±

KullanÄ±cÄ±yla yaptÄ±ÄŸÄ± sohbetleri hatÄ±rlayabilen (hafÄ±zalÄ±), yerel bir LLM tarafÄ±ndan desteklenen etkileÅŸimli bir chatbot oluÅŸturmaktÄ±r.

## ğŸ› ï¸ KullanÄ±lan Teknolojiler

- **Model ve Orkestrasyon:** `LangChain`, `Ollama` (LLaMA 3.2)
- **Backend (API Sunucusu):** `FastAPI`
- **Frontend (KullanÄ±cÄ± ArayÃ¼zÃ¼):** `Streamlit`
- **Ana Konsept:** HafÄ±zalÄ± Sohbet Zinciri (Conversational Chain with Memory)

## ğŸ“‚ Dosya YapÄ±sÄ± ve GÃ¶revleri

-   **`main.py`**: `FastAPI` sunucusu oluÅŸturur. `LangChain` ve `Ollama` kullanarak, oturum bazlÄ± hafÄ±za yÃ¶netimi yapan bir sohbet zinciri kurar. KullanÄ±cÄ±dan gelen mesajlarÄ± iÅŸleyen bir `/chat` endpoint'i sunar.
-   **`ui.py`**: `Streamlit` ile klasik bir sohbet arayÃ¼zÃ¼ oluÅŸturur. KullanÄ±cÄ±nÄ±n girdiÄŸi her mesajÄ± FastAPI'deki `/chat` endpoint'ine gÃ¶nderir, gelen cevabÄ± alÄ±r ve sohbet geÃ§miÅŸini ekranda gÃ¶sterir.
-   *(Not: Bu projede `model_train.py` yoktur Ã§Ã¼nkÃ¼ Ã¶nceden eÄŸitilmiÅŸ bir LLM kullanÄ±lmaktadÄ±r. `client_test.py` ise arayÃ¼z Ã¼zerinden kolayca test edilebildiÄŸi iÃ§in bu projeye dahil edilmemiÅŸtir.)*

## NasÄ±l Ã‡alÄ±ÅŸtÄ±rÄ±lÄ±r?

Bu projeyi Ã§alÄ±ÅŸtÄ±rmak iÃ§in iki ayrÄ± terminal penceresine ihtiyacÄ±nÄ±z olacaktÄ±r: biri backend (FastAPI) iÃ§in, diÄŸeri frontend (Streamlit) iÃ§in.

**Ã–n KoÅŸullar:**
- Ana `MLOps_Project` klasÃ¶rÃ¼nde kurulum adÄ±mlarÄ±nÄ± tamamladÄ±ÄŸÄ±nÄ±zdan, sanal ortamÄ±n (`venv`) aktif olduÄŸundan ve **Ollama'nÄ±n Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan** emin olun.

**1. AdÄ±m: Backend Sunucusunu BaÅŸlatma**
   - Yeni bir terminal aÃ§Ä±n.
   - Ana `MLOps_Project` dizinine gidin ve sanal ortamÄ± aktif edin:
     ```bash
     cd path/to/MLOps_Project
     .\venv\Scripts\activate
     ```
   - FastAPI sunucusunu Ã§alÄ±ÅŸtÄ±rÄ±n:
     ```bash
     uvicorn 3_chatbot_with_LLM.main:app --reload
     ```
   - Sunucu ÅŸimdi `http://127.0.0.1:8000` adresinde Ã§alÄ±ÅŸÄ±yor olmalÄ±.

**2. AdÄ±m: Frontend ArayÃ¼zÃ¼nÃ¼ BaÅŸlatma**
   - **Ä°kinci** bir terminal aÃ§Ä±n.
   - Yine ana `MLOps_Project` dizinine gidin ve sanal ortamÄ± aktif edin:
     ```bash
     cd path/to/MLOps_Project
     .\venv\Scripts\activate
     ```
   - Streamlit arayÃ¼zÃ¼nÃ¼ Ã§alÄ±ÅŸtÄ±rÄ±n:
     ```bash
     streamlit run 3_chatbot_with_LLM/ui.py
     ```
   - TarayÄ±cÄ±nÄ±zda otomatik olarak yeni bir sekme aÃ§Ä±lacak ve chatbot ile sohbet etmeye baÅŸlayabileceksiniz.

---
---

## ENG:
# Project 3: LLM-Powered Chatbot (MLOps with LLMs)

This project demonstrates how the MLOps lifecycle is adapted for the modern Large Language Models (LLM) domain. In this application, there is no traditional training step like `model_train.py`; instead, it utilizes a pre-trained LLaMA model running locally (via `Ollama`). Memory management is handled by `LangChain`, a chat API is created with `FastAPI`, and the user interface is served with `Streamlit`.

---

## Project Goal

To create an interactive chatbot, powered by a local LLM, that can remember the context of the conversation (i.e., has memory).

## ğŸ› ï¸ Technologies Used

- **Model and Orchestration:** `LangChain`, `Ollama` (LLaMA 3.2)
- **Backend (API Server):** `FastAPI`
- **Frontend (User Interface):** `Streamlit`
- **Core Concept:** Conversational Chain with Memory

## ğŸ“‚ File Structure and Roles

-   **`main.py`**: Creates a `FastAPI` server. It sets up a conversational chain using `LangChain` and `Ollama` that manages session-based memory. It serves a `/chat` endpoint that processes incoming user messages.
-   **`ui.py`**: Creates a classic chat interface with `Streamlit`. It sends each user message to the FastAPI `/chat` endpoint, receives the response, and displays the entire conversation history on the screen.
-   *(Note: This project does not have a `model_train.py` because it uses a pre-trained LLM. A `client_test.py` is also omitted as it can be easily tested via the user interface.)*

## How to Run

You will need two separate terminal windows to run this project: one for the backend (FastAPI) and one for the frontend (Streamlit).

**Prerequisites:**
- Ensure you have completed the setup steps in the main `MLOps_Project` directory, that the virtual environment (`venv`) is activated, and that **Ollama is running**.

**Step 1: Start the Backend Server**
   - Open a new terminal.
   - Navigate to the main `MLOps_Project` directory and activate the virtual environment:
     ```bash
     cd path/to/MLOps_Project
     .\venv\Scripts\activate
     ```
   - Run the FastAPI server:
     ```bash
     uvicorn 3_chatbot_with_LLM.main:app --reload
     ```
   - The server should now be running at `http://127.0.0.1:8000`.

**Step 2: Start the Frontend Interface**
   - Open a **second** terminal.
   - Again, navigate to the main `MLOps_Project` directory and activate the virtual environment:
     ```bash
     cd path/to/MLOps_Project
     .\venv\Scripts\activate
     ```
   - Run the Streamlit interface:
     ```bash
     streamlit run 3_chatbot_with_LLM/ui.py
     ```
   - A new tab will automatically open in your browser, and you can start chatting with the bot.
